%% Los cap'itulos inician con \chapter{T'itulo}, estos aparecen numerados y
%% se incluyen en el 'indice general.
%%
%% Recuerda que aqu'i ya puedes escribir acentos como: 'a, 'e, 'i, etc.
%% La letra n con tilde es: 'n.

\chapter{Introducción}

La teledetección constituye una herramienta de gran utilidad para el desarrollo de sistemas de prevención, seguimiento y evaluación de superficie terrestre. Se define como  las técnicas que permiten adquirir imágenes (y otras formas de organizar datos: señales, series de tiempo, etc.) de la  superficie terrestre desde sensores instalados en plataformas espaciales o aereotransportadas. 
Este proceso se basa en la detección y medición de radiación electromagnética emitida ya sea de manera natural o a partir de una fuente artificial emisora.
La energía es captada por sensores y permite identificar objetos por sus propiedades físicas y sustancias que los componen. 
Estos sensores captan la radiación comprendida en varias zonas del espectro electromagnético. 

%%% ACF Usá la librería siunitx para tipear unidades
La parte del espectro electromagnético usada en teledetección se extiende aproximadamente desde $\SI{e-1}{\micro\meter}$ hasta $\SI{e9}{\micro\meter}$.

Esta técnica de teledetección comprende dos procesos. 
Uno de estos procesos es la adquisición de información de la superficie terrestre captando la radiación electromagnética emitida o reflejada por ésta. 
El otro es el almacenamiento de la información obtenida para su posterior procesamiento en virtud de interpretarla y utilizarla. 

El sensor remoto recibe la radiación que proviene de los objetos situados sobre la superficie terrestre. 
En función de la forma en la que se refleje esta radiación podremos clasificar los distintos tipos de terrenos.

%\begin{figure}[hbt]
%	\centering    
%	\includegraphics[scale=0.7]{../../Figures/Tesis/Capitulo4/Backscatter.pdf}
%	\caption{\label{Backscatter}Radiación reflejada por distintos objetos en la superficie terrestre.}
%\end{figure} 

Los sistemas de teledetección se pueden dividir principalmente en dos grupos: los teledetectores activos y los pasivos. 

Los sensores activos tienen su propia fuente de energía para iluminar los objetos que observan. 
Ellos emiten radiación en la dirección del objetivo a ser investigado, detectan y miden la radiación que es reflejada o retrodispersada desde el objetivo. 
El radar es un ejemplo de sensor activo.

Los sensores pasivos, por otra parte, detectan la radiación electromagnética emitida o reflejada de fuentes naturales, no necesitan una fuente de energía externa. 
La luz del sol reflejada es la fuente más común de la radiación medida por los sensores pasivos. 
Un ejemplo de sensor pasivo es el espectrómetro óptico o espectroscopio, que es un instrumento que sirve para medir las propiedades de la luz en una determinada porción del espectro electromagnético. 


%Entre las ventajas que ofrece la Teledetección, destaca su alta periodicidad temporal, por lo que se facilita el seguimiento de aquellas variables ambientales sometidas a una intensa dinámica. Además, gracias a la teledetección ambiental se brinda la posibilidad de obtener información de grandes superficies de territorio en poco tiempo, de tal manera que extensas regiones pueden ser muestreadas en su totalidad en pocos días.

Los sistemas de Radar (\textit{radio detection and ranging}: detección y medición de distancias por radio) son instrumentos que, a través de ondas electromagnéticas, detectan un objeto e indican su distancia y posición. Estos instrumentos miden la respuesta del terreno a la radiación electromagnética emitida en forma de pulsos, el valor de esta respuesta es almacenada para su posterior procesamiento y se utiliza para formar una imagen de la zona de interés. Un Radar de Apertura Sintética (\textit{Synthetic Aperture Radar} -- SAR) es un tipo de sistema que consiste en procesar, mediante algoritmos, la información capturada por la antena del radar. Mediante este proceso se consigue el mismo rendimiento que se obtendría si se utilizara una antena mucho más grande que la que tiene en realidad, por eso se llaman de apertura sintética. 

Los dispositivos de captura de imágenes que emplean iluminación coherente, como sucede en las imágenes de ultrasonido B, laser y SAR, introducen un ruido que es propio del sistema de captura de la imagen. 
Este ruido, llamado \textit{speckle}, no es gaussiano ni aditivo y, por lo tanto, diferente al ruido que se observa en imágenes ópticas. 
Existen diferentes técnicas para disminuir la presencia de este ruido, pero como contrapartida la imagen pierde resolución.

La utilización de modelos estadísticos ha sido una herramienta fundamental para analizar e interpretar datos con ruido speckle. 
Se han presentado varias distribuciones para modelar este tipo de imágenes. Las distribuciones Lognormal y Weibull fueron introducidas por \citet{oliverquegan98} para caracterizar datos de alta resolución, mientras que \citet{Oliver1993} estudió las distribuciones \textit{K} y Weibull. 
\citet{Tison2004} presentaron la distribución de Fisher para modelar varios tipos de áreas. 
\citet{Li2011} propusieron una distribución Gamma generalizada para modelar imágenes SAR; esa distribución tiene las leyes Weibull, \textit{K} y Fisher, entre otras, como casos particulares. 
Cabe señalar que la distribución $\mathcal{G}^0$ está relacionada con la distribución de Fisher, como muestran \citet{MejailJacoboFreryBustos:IJRS}.

\citet{oliverquegan98} proponen modelar datos provenientes de un sistema de iluminación por radiación coherente, como son los datos SAR, con un modelo multiplicativo que considera que el valor observado en cada celda de la imagen es una variable aleatoria $Z$ que resulta del producto de dos variables aleatorias independientes: una correspondiente a la retrodispersión $X$ (que es lo que observaríamos sin la presencia del ruido speckle) y la otra correspondiente al ruido speckle $Y$ (que es inherente a todo sistema de captura de imágenes con iluminación coherente). 

En los últimos años, se ha utilizado exitosamente el modelo $\mathcal{G}^0$: $\mathcal{G}_A^0$ para datos de amplitud y $\mathcal G_I^0$ para datos de intensidad, porque tiene la capacidad de discriminar áreas muy texturadas o extremadamente texturadas mejor que otros modelos. Este modelo fue propuesto por \citet{Frery97} y está caracterizado por tres parámetros; el parámetro $\alpha$ que explica la textura, el parámetro $\gamma $ que informa sobre el brillo de la imagen y el número de looks $L$ que está relacionado con la relación señal-ruido.
Debido a esta interpretabilidad, es crucial obtener estimaciones de calidad de dichos parámetros, en particular para $\alpha$.

Varios autores han estudiado el problema de la estimación de parámetros para la familia $\mathcal G^0$. \citet{Freitas2005} usaron el primer y segundo momento para estimar los parámetros de textura y escala en el caso de datos de intensidad.
 \citet{VasconcellosFrerySilva:CompStat} cuantifican el error en la estimación del parámetro de textura para la distribución $\mathcal G_A^0$ y proponen una técnica analítica para mejorar la estimación a través de una corrección de segundo orden en el sesgo de la estimación del parámetro de textura. \citet{Li2011} obtuvieron una expresión para los estimadores de la distribución Gamma generalizada basada en cumulantes de segundo tipo y una aproximación de segundo orden de la función Polygamma. \citet{CribariFrerySilva:CSDA} implementaron técnicas de remuestreo para mejorar la estimación de parámetros para el modelo $\mathcal G_I^0$.

 \citet{nicolas2002} propusieron el método basado en Logmomentos y Logcumulantes para estimar los parámetros de una distribución. Estos métodos dependen de la relación entre los momentos de la distribución y su transformada de Mellin. La misma idea fue aplicada por \citet{khan2014} para estimar los parámetros del modelo $\mathcal{G}^0$ multivariado en el caso de datos SAR polarimétricos y también fue utilizada por \citet{Tison2004} para la estimación del modelo $\mathcal{G}^0_A$.

Unas de las propiedades deseables para un estimador es su robustez, esto es, su capacidad para dar buenas estimaciones aún en presencia de datos atípicos. \citet{BustosFreryLucini:Mestimators:2001} y \citet{AllendeFreryetal:JSCS:05} propusieron M y AM estimadores para mejorar el comportamiento del estimador de Máxima Verosimilitud bajo contaminación. Ellos mostraron que su propuesta, si bien supera la performance del estimador de Máxima Verosimilitud, presenta problemas numéricos especialmente para el caso de muestras de pequeño tamaño.

Por otro lado, la teoría de la información ha sido aplicada a los métodos de estadística y probabilidades con éxito. \citet{Shannon1948} definió la información $I(X,Y)$ entre las variables aleatorias $X$ e $Y$ como una divergencia calculada entre sus densidades de probabilidad. 
Estas divergencias fueron ampliamente estudiadas por \citet{KullbackLeibler1951} y por \citet{renyi1961} entre otros autores. \citet{pardo2005statistical} y \citet{Basu2011} han proporcionado tratamientos útiles en inferencia estadística basada en medidas de divergencia.

%El concepto de divergencias como distancias estocásticas se describe detalladamente en \citet{Liese2006}, así como también sus propiedades.  
% Mejorar las referencia en eventos con artigos en periódicos
Este tipo de divergencias posee múltiples aplicaciones en procesamiento de señales e imágenes. 
\citet{Aviyente2007} utilizan medidas de divergencia para cuantificar la interacción entre diferentes sitios neuronales.
\citet{5599869} aplican estas medidas en el análisis de imágenes médicas.
\citet{1246862} las utilizan en clasificación de texturas, mientras que
\citet{6377288} y \citet{ClassificationPolSARSegmentsMinimizationWishartDistances} en detección automática de regiones con diferente grado de rugosidad en imágenes SAR.


Los estimadores de mínima distancia (\textit{Minimum Distance Estimators } -- MDE) son una  alternativa con buenas propiedades para el problema de la estimación de parámetros.
Surgen de la idea de encontrar un estimador que sea el valor que minimiza la medida de la distancia entre las funciones de distribución empírica y teórica.
\citet{wolfowitz1953, wolfowitz1957} estudió esta clase de estimadores y demostró que, bajo condiciones generales, estos estimadores son fuertemente consistentes. 
\citet{Boos1981} estudió la distancia ponderada de Cramer-von Mises entre la función de distribución empírica y el modelo verdadero.
El autor mostró que estos estimadores son consistentes y asintóticamente eficientes bajo cierta función de peso. \citet{HettmanSperger1994} estudiaron estimadores de mínima distancia no pesados aplicados a un modelo de posición-escala a partir de la distancia de Cramer-von Mises. Los autores demostraron que estos estimadores son asintóticamente normales y tienen buenas propiedades de eficiencia y robustez. \citet{beran1977} propone un MDE estimador utilizando la distancia de Hellinger entre un modelo teórico y un estimador de densidad no paramétrico utilizando núcleos simétricos y mostró que este estimador es asintóticamente eficiente bajo ciertas familias paramétricas de densidades. \citet{parr1982} demostraron que, considerando ciertas condiciones, el estimador MDE entre la función de distribución empírica y la función de distribución teórica es fuertemente consistente.

\citet{cao1995minimum} propusieron minimizar una distancia entre la función de densidad teórica y un estimador de la función de densidad subyacente utilizando el estimador de núcleo simétrico clásico para obtener estimadores de distancia mínima. Demostraron, siguiendo las ideas de \citet{parr1982}, la consistencia fuerte de estos estimadores bajo ciertas consideraciones, y también estudiaron su normalidad asintótica para el caso de la métrica $ L^2 $.

%Gambini et al. \citet{gambini2015} propusieron un estimador del parámetro de textura de la distribución $\mathcal{G}_I^0$ combinando MDE con distancias estocásticas. 
%combining MDEs with stochastic distances. Ellos prponen minimizar la distancia triangular entre la función de densidad teórica y una estimación no paramétrica de la función de densidad subyacente utilizando núcleos asimétricos, en este caso, utilizaron el kernel gaussiano inverso. Los autores compararon el desempeño del estimador propueston con el obtenido por logcumulación y Máxima Verosimilitud, y obtuvieron buenos resultados
%en términos de error cuadrático medio, sesgo, consistencia y robustez. 

Dentro de los estimadores no paramétricos de la función de densidad subyacente se encuentran los estimadores de kernel clásico, con kernel simétrico que son populares en la estimación de la función de densidad.  \citet{Silverman1986} es una referencia clásica para este tipo de estimadores.

Sin embargo, si la función de densidad a estimar tiene soporte acotado o semiacotado, estos estimadores pueden dar estimaciones sesgadas en los bordes porque asignan probabilidad positiva fuera del soporte de la función. 
Una alternativa para mejorar esto es utilizar núcleos asimétricos. \citet{chen1999, chensx2000} presentan los núcleos Beta y Gamma, \citet {Scaillet2004} introduce los núcleos Inverso Gaussianos (IG) y Recíproco Inverso Gaussiano (RIG), \citet {bouezmarni2005} demuestran propiedades teóricas de los núcleos Gamma, IG y RIG, \citet{Jin2003}  proponen los núcleos Birnbaum-Saunders (BS) y Lognormal (LN). Es interesante señalar que  estos estimadores varían su forma de acuerdo con la observación, una característica que permite obtener diferentes grados de suavizamiento sin incurrir en los problemas antes mencionados \citep{Scaillet2004}. 

En esta tesis se propone un nuevo  estimador de los parámetros de la distribución $\mathcal{G}_I^0$ definido como el punto del espacio paramétrico que minimiza la distancia estocástica que existe entre la función de densidad teórica $\mathcal{G}_I^0$ y una estimación no paramétrica de la función de densidad subyacente utilizando núcleos asimétricos. Se muestra que la distancia triangular es una buena elección para tratar este problema y se prueba la convergencia fuerte de este estimador. Asimismo, a través de simulaciones Monte Carlo, se analizan:
\begin{itemize}
	\item El desempeño de este estimador para pequeñas muestras comparándolo, en términos de sesgo y error cuadrático medio y tasa de convergencia, con los estimadores presentes en la literatura.
	\item La robustez del estimador propuesto bajo diferentes esquemas de contaminación.
\end{itemize}  

Esta tesis está organizada de la siguiente manera:
\begin{itemize}
	\item En el capítulo~\ref{Radar} presentamos conceptos de teledetección y los radares de apertura sintética dando una breve reseña de principales sensores SAR. Presentamos también la generación de imágenes SAR y  la formación del ruido speckle.
	\item En el capítulo~\ref{modeloG0} se introduce el modelo multiplicativo, base de la metodología adoptada para explicar el comportamiento de las imágenes SAR. Presentamos las distribuciones de cada una de las variables que forman parte del modelo multiplicativo y, finalmente, presentamos la familia de distribuciones $\mathcal{G}_I^0$ objeto de estudio en esta tesis.
	\item En el capítulo~\ref{metodologia}  se describen dos metodologías de estimación de la función de densidad, paramétrica y no paramétrica. Damos una descripción de cada una de estas metodologías y presentamos el principal aporte de esta tesis, que es la propuesta de un nuevo método de estimación para los parámetros del modelo $\mathcal G_I^0$.
	\item En el capítulo~\ref{ResultadosEmpiricos} se presenta el estudio realizado, a través de simulaciones Monte Carlo, del estimador propuesto. Se muestran los resultados obtenidos de estas simulaciones  con datos contaminados y sin contaminar, y la aplicación de estos métodos en imágenes reales.
	\item En el capítulo~\ref{ResultadosTeoricos} se presenta resultados de convergencia del estimador propuesto.
	\item En el capítlo~\ref{Conclusiones} se presentan las conclusiones finales de este trabajo.
	
\end{itemize}